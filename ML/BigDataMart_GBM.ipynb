{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "sales_data = pd.read_csv('Train_UWu5bXk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanbytype=sales_data.groupby('Item_Type').Item_Weight.transform('mean')\n",
    "sales_data['Item_Weight'].fillna(meanbytype,inplace=True)\n",
    "sales_data['Outlet_Size'].fillna('Missing',inplace=True)\n",
    "# get the age of Outlet\n",
    "import datetime \n",
    "datetime.datetime.now().year\n",
    "sales_data['Outlet_Age']=sales_data.Outlet_Establishment_Year.sub(datetime.datetime.now().year).abs()\n",
    "#Encode outlet size ordinally\n",
    "scale_size={'Missing':-1,'Small':1,'Medium':2,'High':3}\n",
    "sales_data['Outlet_Size']=sales_data['Outlet_Size'].replace(scale_size)\n",
    "#clean fat content column\n",
    "sales_data.Item_Fat_Content=sales_data.Item_Fat_Content.str.upper()\n",
    "scale_fat_content={'LOW FAT':0,'LF':0,'REGULAR':1,'REG':1,'HIGH FAT':2,'HF':2}\n",
    "sales_data['Item_Fat_Content']=sales_data['Item_Fat_Content'].replace(scale_fat_content)\n",
    "sales_data.Outlet_Location_Type=pd.to_numeric(sales_data.Outlet_Location_Type.str.replace('Tier ',''))\n",
    "scale_fat_outlet_type={'Grocery Store':0,'Supermarket Type1':1,'Supermarket Type2':2,'Supermarket Type3':3}\n",
    "sales_data['Outlet_Type']=sales_data['Outlet_Type'].replace(scale_fat_outlet_type)\n",
    "sales_data=pd.concat([sales_data,pd.get_dummies(sales_data.Item_Type,prefix='Item_Type')],axis=1)\n",
    "sales_data['Item_Ident_prefix']=sales_data.Item_Identifier.str.slice(start=0,stop=2)\n",
    "sales_data=pd.concat([sales_data,pd.get_dummies(sales_data.Item_Ident_prefix,prefix='Item_Identifier')],axis=1)\n",
    "sales_data=pd.concat([sales_data,pd.get_dummies(sales_data.Outlet_Identifier,prefix='Outlet_Identifier')],axis=1)\n",
    "sales_data=sales_data.drop(['Item_Identifier','Outlet_Establishment_Year','Item_Type','Outlet_Identifier','Item_Ident_prefix'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=sales_data['Item_Outlet_Sales'].values\n",
    "X=sales_data.drop(['Item_Outlet_Sales'],axis=1).values\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# ss=StandardScaler()\n",
    "# X_std=ss.fit_transform(X)\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# pf=PolynomialFeatures(3)\n",
    "# X_Poly_test=pf.fit_transform(X_std)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 1055.1424954648342 Test Error 1095.8731892139044\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rfc=GradientBoostingRegressor(loss='ls',\n",
    "  max_depth= 3,\n",
    "  min_samples_leaf= 100,\n",
    "  min_samples_split= 799,\n",
    "  n_estimators=58)\n",
    "rfc.fit(xtrain,ytrain)\n",
    "# print(rfc.score(xtrain,ytrain))\n",
    "# print(rfc.score(xtest,ytest))\n",
    "pred_train=rfc.predict(xtrain)\n",
    "pred_test=rfc.predict(xtest)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lin_rmse_train = np.sqrt(mean_squared_error(ytrain,pred_train))\n",
    "lin_rmse_test=np.sqrt(mean_squared_error(ytest,pred_test))\n",
    "print(\"Train Error\",lin_rmse_train,\"Test Error\",lin_rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Item_Weight', 'Item_Fat_Content', 'Item_Visibility', 'Item_MRP',\n",
       "       'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type',\n",
       "       'Item_Outlet_Sales', 'Outlet_Age', 'Item_Type_Baking Goods',\n",
       "       'Item_Type_Breads', 'Item_Type_Breakfast', 'Item_Type_Canned',\n",
       "       'Item_Type_Dairy', 'Item_Type_Frozen Foods',\n",
       "       'Item_Type_Fruits and Vegetables', 'Item_Type_Hard Drinks',\n",
       "       'Item_Type_Health and Hygiene', 'Item_Type_Household', 'Item_Type_Meat',\n",
       "       'Item_Type_Others', 'Item_Type_Seafood', 'Item_Type_Snack Foods',\n",
       "       'Item_Type_Soft Drinks', 'Item_Type_Starchy Foods',\n",
       "       'Item_Identifier_DR', 'Item_Identifier_FD', 'Item_Identifier_NC',\n",
       "       'Outlet_Identifier_OUT010', 'Outlet_Identifier_OUT013',\n",
       "       'Outlet_Identifier_OUT017', 'Outlet_Identifier_OUT018',\n",
       "       'Outlet_Identifier_OUT019', 'Outlet_Identifier_OUT027',\n",
       "       'Outlet_Identifier_OUT035', 'Outlet_Identifier_OUT045',\n",
       "       'Outlet_Identifier_OUT046', 'Outlet_Identifier_OUT049'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sales_data.hist(bins=50,figsize=(30,30),xlabelsize=20, ylabelsize=25 , grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 50})\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# attributes = ['Item_MRP' , 'Item_Visibility' , 'Item_Outlet_Sales']# , 'Outlet_Establishment_Year']\n",
    "# help(scatter_matrix)\n",
    "scat=scatter_matrix(sales_data[['Item_Visibility','Item_Outlet_Sales']], figsize=(50,40))\n",
    "# scat=scatter_matrix(sales_data[['Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type','Item_Outlet_Sales']], figsize=(50,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>train_error</th>\n",
       "      <th>test_error</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ls</td>\n",
       "      <td>1054.954495</td>\n",
       "      <td>1095.845962</td>\n",
       "      <td>40.891468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lad</td>\n",
       "      <td>1072.175265</td>\n",
       "      <td>1104.952226</td>\n",
       "      <td>32.776960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huber</td>\n",
       "      <td>1057.864629</td>\n",
       "      <td>1100.502951</td>\n",
       "      <td>42.638322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quantile</td>\n",
       "      <td>1749.628140</td>\n",
       "      <td>1780.929738</td>\n",
       "      <td>31.301598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Parameter  train_error   test_error       diff\n",
       "0        ls  1054.954495  1095.845962  40.891468\n",
       "1       lad  1072.175265  1104.952226  32.776960\n",
       "2     huber  1057.864629  1100.502951  42.638322\n",
       "3  quantile  1749.628140  1780.929738  31.301598"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n_estimators=[0.01,0.25,0.5,0.75,1]\n",
    "n_estimators=['ls', 'lad', 'huber', 'quantile']\n",
    "df=pd.DataFrame(columns=['Parameter','train_error','test_error','diff'])\n",
    "for val in n_estimators:#range(2,300,10):\n",
    "    rfc=GradientBoostingRegressor(loss=val,n_estimators=60,max_depth=3,min_samples_split=775,min_samples_leaf=102)\n",
    "    rfc.fit(xtrain,ytrain)\n",
    "    pred_train=rfc.predict(xtrain)\n",
    "    pred_test=rfc.predict(xtest)\n",
    "    lin_rmse_train = np.sqrt(mean_squared_error(ytrain,pred_train))\n",
    "    lin_rmse_test=np.sqrt(mean_squared_error(ytest,pred_test))\n",
    "#     print('Parameter',val,'train_error',lin_rmse_train,'test_error',lin_rmse_test)\n",
    "    df=df.append(pd.DataFrame({'Parameter':[val],'train_error':[lin_rmse_train],'test_error':[lin_rmse_test],'diff':[lin_rmse_test-lin_rmse_train]}),ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import regression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def dt_param_grid(X, y, nfolds):\n",
    "    \n",
    "    param_grid = {'loss':['ls', 'lad', 'huber', 'quantile'],\n",
    "                  'max_depth': [2,3,4,5,6], \n",
    "                  'min_samples_split': [760,770,775,780,799],\n",
    "                  'min_samples_leaf':[92,95,100,102,105],\n",
    "                  'n_estimators':[58,59,60,61,62]\n",
    "                 }\n",
    "    ##grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=nfolds)\n",
    "    \n",
    "    grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    y_true, y_pred = y, grid_search.predict(X)\n",
    "    print(regression.mean_squared_error(y_true, y_pred))\n",
    "    print()\n",
    "    \n",
    "    return grid_search.best_params_,grid_search.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'ls', 'max_depth': 3, 'min_samples_leaf': 100, 'min_samples_split': 799, 'n_estimators': 58}\n",
      "Detailed classification report:\n",
      "\n",
      "1126914.2280779937\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'loss': 'ls',\n",
       "  'max_depth': 3,\n",
       "  'min_samples_leaf': 100,\n",
       "  'min_samples_split': 799,\n",
       "  'n_estimators': 58},\n",
       " 0.6129835444896026)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_param_grid(X,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_test=pd.read_csv('Test_u94Q5KV.csv')\n",
    "sales_data_tmp=sales_data_test\n",
    "meanbytype=sales_data_test.groupby('Item_Type').Item_Weight.transform('mean')\n",
    "sales_data_test['Item_Weight'].fillna(meanbytype,inplace=True)\n",
    "sales_data_test['Outlet_Size'].fillna('Missing',inplace=True)\n",
    "# get the age of Outlet\n",
    "import datetime \n",
    "datetime.datetime.now().year\n",
    "sales_data_test['Outlet_Age']=sales_data_test.Outlet_Establishment_Year.sub(datetime.datetime.now().year).abs()\n",
    "#Encode outlet size ordinally\n",
    "scale_size={'Missing':1,'Small':1,'Medium':2,'High':3}\n",
    "sales_data_test['Outlet_Size']=sales_data_test['Outlet_Size'].replace(scale_size)\n",
    "#clean fat content column\n",
    "sales_data_test.Item_Fat_Content=sales_data_test.Item_Fat_Content.str.upper()\n",
    "scale_fat_content={'LOW FAT':0,'LF':0,'REGULAR':1,'REG':1}\n",
    "sales_data_test['Item_Fat_Content']=sales_data_test['Item_Fat_Content'].replace(scale_fat_content)\n",
    "sales_data_test.Outlet_Location_Type=pd.to_numeric(sales_data_test.Outlet_Location_Type.str.replace('Tier ',''))\n",
    "scale_fat_outlet_type={'Grocery Store':0,'Supermarket Type1':1,'Supermarket Type2':2,'Supermarket Type3':3}\n",
    "sales_data_test['Outlet_Type']=sales_data_test['Outlet_Type'].replace(scale_fat_outlet_type)\n",
    "# sales_data_test.Item_Type=LabEncoder.fit_transform(sales_data_test.Item_Type)\n",
    "sales_data_test=pd.concat([sales_data_test,pd.get_dummies(sales_data_test.Item_Type,prefix='Item_Type')],axis=1)\n",
    "# sales_data_test=pd.concat([sales_data_test,pd.get_dummies(sales_data_test.Item_Type,prefix='Item_Identifier')],axis=1)\n",
    "sales_data_test['Item_Ident_prefix']=sales_data_test.Item_Identifier.str.slice(start=0,stop=2)\n",
    "sales_data_test=pd.concat([sales_data_test,pd.get_dummies(sales_data_test.Item_Ident_prefix,prefix='Item_Identifier')],axis=1)\n",
    "# sales_data_test['Item_Ident_post']=sales_data_test.Item_Identifier.str.slice(start=2,stop=3)\n",
    "sales_data_test=pd.concat([sales_data_test,pd.get_dummies(sales_data_test.Outlet_Identifier,prefix='Outlet_Identifier')],axis=1)\n",
    "sales_data_test=sales_data_test.drop(['Item_Identifier','Outlet_Establishment_Year','Item_Type','Outlet_Identifier','Item_Ident_prefix'],axis=1)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# ss=StandardScaler()\n",
    "# X_std=ss.fit_transform(sales_data_test)\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# pf=PolynomialFeatures(3)\n",
    "# X_Poly_test=pf.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 1055.1424954648342 Test Error 1095.8731892139044\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rfc=GradientBoostingRegressor(loss='ls',\n",
    "  max_depth= 3,\n",
    "  min_samples_leaf= 100,\n",
    "  min_samples_split= 799,\n",
    "  n_estimators=58)\n",
    "rfc.fit(xtrain,ytrain)\n",
    "# print(rfc.score(xtrain,ytrain))\n",
    "# print(rfc.score(xtest,ytest))\n",
    "pred_train=rfc.predict(xtrain)\n",
    "pred_test=rfc.predict(xtest)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lin_rmse_train = np.sqrt(mean_squared_error(ytrain,pred_train))\n",
    "lin_rmse_test=np.sqrt(mean_squared_error(ytest,pred_test))\n",
    "print(\"Train Error\",lin_rmse_train,\"Test Error\",lin_rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 500.8595459622268 Test Error 1175.135881473382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr=RandomForestRegressor()\n",
    "rfr.fit(xtrain,ytrain)\n",
    "pred_train=rfr.predict(xtrain)\n",
    "pred_test=rfr.predict(xtest)\n",
    "lin_rmse_train = np.sqrt(mean_squared_error(ytrain,pred_train))\n",
    "lin_rmse_test=np.sqrt(mean_squared_error(ytest,pred_test))\n",
    "print(\"Train Error\",lin_rmse_train,\"Test Error\",lin_rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestRegressor in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  RandomForestRegressor(n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      " |  \n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and uses averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |         The default value of ``n_estimators`` will change from 10 in\n",
      " |         version 0.20 to 100 in version 0.22.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"mse\")\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      " |      absolute error.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole datset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, optional (default=False)\n",
      " |      whether to use out-of-bag samples to estimate\n",
      " |      the R^2 on unseen data.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_prediction_ : array of shape = [n_samples]\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  \n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0,\n",
      " |  ...                              n_estimators=100)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      " |             max_features='auto', max_leaf_nodes=None,\n",
      " |             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |             min_samples_leaf=1, min_samples_split=2,\n",
      " |             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      " |             oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(regr.feature_importances_)\n",
      " |  [0.18146984 0.81473937 0.00145312 0.00233767]\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-8.32987858]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  The default value ``max_features=\"auto\"`` uses ``n_features`` \n",
      " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      " |  [1], whereas the former was more recently justified empirically in [2].\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized \n",
      " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix instead, shape = (n_samples,\n",
      " |          n_samples_fitted], where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100,200,5):\n",
    "    rfr=RandomForestRegressor(warm_start=True,n_estimators =100,max_depth=6,min_samples_split=100,min_samples_leaf=6)\n",
    "    rfr.fit(xtrain,ytrain)\n",
    "    pred_train=rfr.predict(xtrain)\n",
    "    pred_test=rfr.predict(xtest)\n",
    "    lin_rmse_train = np.sqrt(mean_squared_error(ytrain,pred_train))\n",
    "    lin_rmse_test=np.sqrt(mean_squared_error(ytest,pred_test))\n",
    "    print(\"Param\",i,\"Train Error\",lin_rmse_train,\"Test Error\",lin_rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061.5621640196082"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc=GradientBoostingRegressor(loss='ls',\n",
    "  max_depth= 3,\n",
    "  min_samples_leaf= 100,\n",
    "  min_samples_split= 799,\n",
    "  n_estimators=58)\n",
    "rfc.fit(X,y)\n",
    "pred=rfc.predict(X)\n",
    "lin_rmse_train = np.sqrt(mean_squared_error(y,pred))\n",
    "lin_rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 1065.4723581077258\n"
     ]
    }
   ],
   "source": [
    "rfr=RandomForestRegressor(n_estimators =100,max_depth=5,min_samples_split=100,min_samples_leaf=6)\n",
    "rfr.fit(X,y)\n",
    "pred=rfc.predict(X)\n",
    "lin_rmse_train = np.sqrt(mean_squared_error(y,pred))\n",
    "print(\"Train Error\",lin_rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_tmp['Item_Outlet_Sales']=rfr.predict(sales_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Output=sales_data_tmp[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']]\n",
    "final_Output.to_csv('BigDataMart_Predicted3.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
