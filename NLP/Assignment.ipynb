{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=['computer','planet','day','country','environment','money','weather','network','nature','FBI','stock','baby','television','game','currency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-7d74a687f3fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mword_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'FirstWord'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'SecondWord'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Similarity'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msyns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#         print(syns.name().split('.')[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmaxsimilarity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m         \u001b[1;31m# split name into lemma, part of speech and synset number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m         \u001b[0mlemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset_index_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1330\u001b[0m         \u001b[0msynset_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynset_index_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "word_dict={'FirstWord':[],'SecondWord':[],'Similarity':[]}\n",
    "for word in word_list:\n",
    "    syns=wn.synset(word)\n",
    "#         print(syns.name().split('.')[0])\n",
    "    maxsimilarity=['',None,0]\n",
    "    maxsimilarity[0]=wn.synset(syns.name())\n",
    "    for syns_inner in wn.synsets(word):\n",
    "        if syns.name().split('.')[0]!=syns_inner.name().split('.')[0]:\n",
    "            word2=wn.synset(syns_inner.name())\n",
    "            similarity=maxsimilarity[0].path_similarity(word2) or 0\n",
    "            if similarity>maxsimilarity[2]:\n",
    "                maxsimilarity[1]=word2\n",
    "                maxsimilarity[2]=similarity\n",
    "#         print(type(maxsimilarity[1]))\n",
    "    if maxsimilarity[1] is not None:\n",
    "        word_dict['FirstWord'].append(maxsimilarity[0].name().split('.')[0])\n",
    "        word_dict['SecondWord'].append(maxsimilarity[1].name().split('.')[0])\n",
    "        word_dict['Similarity'].append(maxsimilarity[2])\n",
    "#         print(word_dict)\n",
    "word_df=pd.DataFrame(word_dict,columns=['FirstWord','SecondWord','Similarity'])\n",
    "word_df.sort_values('Similarity',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')], [Lemma('calculator.n.01.calculator'), Lemma('calculator.n.01.reckoner'), Lemma('calculator.n.01.figurer'), Lemma('calculator.n.01.estimator'), Lemma('calculator.n.01.computer')]]\n",
      "[[Lemma('planet.n.01.planet'), Lemma('planet.n.01.major_planet')], [Lemma('satellite.n.02.satellite'), Lemma('satellite.n.02.planet')], [Lemma('planet.n.03.planet')]]\n",
      "[[Lemma('day.n.01.day'), Lemma('day.n.01.twenty-four_hours'), Lemma('day.n.01.twenty-four_hour_period'), Lemma('day.n.01.24-hour_interval'), Lemma('day.n.01.solar_day'), Lemma('day.n.01.mean_solar_day')], [Lemma('day.n.02.day')], [Lemma('day.n.03.day')], [Lemma('day.n.04.day'), Lemma('day.n.04.daytime'), Lemma('day.n.04.daylight')], [Lemma('day.n.05.day')], [Lemma('day.n.06.day')], [Lemma('day.n.07.day')], [Lemma('sidereal_day.n.01.sidereal_day'), Lemma('sidereal_day.n.01.day')], [Lemma('day.n.09.day')], [Lemma('day.n.10.Day'), Lemma('day.n.10.Clarence_Day'), Lemma('day.n.10.Clarence_Shepard_Day_Jr.')]]\n",
      "[[Lemma('state.n.04.state'), Lemma('state.n.04.nation'), Lemma('state.n.04.country'), Lemma('state.n.04.land'), Lemma('state.n.04.commonwealth'), Lemma('state.n.04.res_publica'), Lemma('state.n.04.body_politic')], [Lemma('country.n.02.country'), Lemma('country.n.02.state'), Lemma('country.n.02.land')], [Lemma('nation.n.02.nation'), Lemma('nation.n.02.land'), Lemma('nation.n.02.country')], [Lemma('country.n.04.country'), Lemma('country.n.04.rural_area')], [Lemma('area.n.01.area'), Lemma('area.n.01.country')]]\n",
      "[[Lemma('environment.n.01.environment')], [Lemma('environment.n.02.environment'), Lemma('environment.n.02.environs'), Lemma('environment.n.02.surroundings'), Lemma('environment.n.02.surround')]]\n",
      "[[Lemma('money.n.01.money')], [Lemma('money.n.02.money')], [Lemma('money.n.03.money')]]\n",
      "[[Lemma('weather.n.01.weather'), Lemma('weather.n.01.weather_condition'), Lemma('weather.n.01.conditions'), Lemma('weather.n.01.atmospheric_condition')], [Lemma('weather.v.01.weather'), Lemma('weather.v.01.endure'), Lemma('weather.v.01.brave'), Lemma('weather.v.01.brave_out')], [Lemma('weather.v.02.weather')], [Lemma('weather.v.03.weather')], [Lemma('weather.v.04.weather')], [Lemma('upwind.s.01.upwind'), Lemma('upwind.s.01.weather')]]\n",
      "[[Lemma('network.n.01.network'), Lemma('network.n.01.web')], [Lemma('network.n.02.network')], [Lemma('net.n.06.net'), Lemma('net.n.06.network'), Lemma('net.n.06.mesh'), Lemma('net.n.06.meshing'), Lemma('net.n.06.meshwork')], [Lemma('network.n.04.network')], [Lemma('network.n.05.network'), Lemma('network.n.05.electronic_network')], [Lemma('network.v.01.network')]]\n",
      "[[Lemma('nature.n.01.nature')], [Lemma('nature.n.02.nature')], [Lemma('nature.n.03.nature')], [Lemma('nature.n.04.nature')], [Lemma('nature.n.05.nature')]]\n",
      "[[Lemma('federal_bureau_of_investigation.n.01.Federal_Bureau_of_Investigation'), Lemma('federal_bureau_of_investigation.n.01.FBI')]]\n",
      "[[Lemma('stock.n.01.stock')], [Lemma('stock.n.02.stock'), Lemma('stock.n.02.inventory')], [Lemma('stock.n.03.stock'), Lemma('stock.n.03.gunstock')], [Lemma('stock_certificate.n.01.stock_certificate'), Lemma('stock_certificate.n.01.stock')], [Lemma('store.n.02.store'), Lemma('store.n.02.stock'), Lemma('store.n.02.fund')], [Lemma('lineage.n.01.lineage'), Lemma('lineage.n.01.line'), Lemma('lineage.n.01.line_of_descent'), Lemma('lineage.n.01.descent'), Lemma('lineage.n.01.bloodline'), Lemma('lineage.n.01.blood_line'), Lemma('lineage.n.01.blood'), Lemma('lineage.n.01.pedigree'), Lemma('lineage.n.01.ancestry'), Lemma('lineage.n.01.origin'), Lemma('lineage.n.01.parentage'), Lemma('lineage.n.01.stemma'), Lemma('lineage.n.01.stock')], [Lemma('breed.n.01.breed'), Lemma('breed.n.01.strain'), Lemma('breed.n.01.stock')], [Lemma('broth.n.01.broth'), Lemma('broth.n.01.stock')], [Lemma('stock.n.09.stock')], [Lemma('stock.n.10.stock'), Lemma('stock.n.10.caudex')], [Lemma('stock.n.11.stock')], [Lemma('stock.n.12.stock'), Lemma('stock.n.12.gillyflower')], [Lemma('malcolm_stock.n.01.Malcolm_stock'), Lemma('malcolm_stock.n.01.stock')], [Lemma('stock.n.14.stock')], [Lemma('stock.n.15.stock')], [Lemma('neckcloth.n.01.neckcloth'), Lemma('neckcloth.n.01.stock')], [Lemma('livestock.n.01.livestock'), Lemma('livestock.n.01.stock'), Lemma('livestock.n.01.farm_animal')], [Lemma('stock.v.01.stock'), Lemma('stock.v.01.carry'), Lemma('stock.v.01.stockpile')], [Lemma('stock.v.02.stock')], [Lemma('stock.v.03.stock')], [Lemma('stock.v.04.stock')], [Lemma('stock.v.05.stock'), Lemma('stock.v.05.buy_in'), Lemma('stock.v.05.stock_up')], [Lemma('stock.v.06.stock')], [Lemma('sprout.v.02.sprout'), Lemma('sprout.v.02.stock')], [Lemma('banal.s.01.banal'), Lemma('banal.s.01.commonplace'), Lemma('banal.s.01.hackneyed'), Lemma('banal.s.01.old-hat'), Lemma('banal.s.01.shopworn'), Lemma('banal.s.01.stock'), Lemma('banal.s.01.threadbare'), Lemma('banal.s.01.timeworn'), Lemma('banal.s.01.tired'), Lemma('banal.s.01.trite'), Lemma('banal.s.01.well-worn')], [Lemma('stock.s.02.stock')], [Lemma('standard.s.05.standard'), Lemma('standard.s.05.stock')]]\n",
      "[[Lemma('baby.n.01.baby'), Lemma('baby.n.01.babe'), Lemma('baby.n.01.infant')], [Lemma('baby.n.02.baby')], [Lemma('child.n.03.child'), Lemma('child.n.03.baby')], [Lemma('baby.n.04.baby')], [Lemma('baby.n.05.baby'), Lemma('baby.n.05.babe'), Lemma('baby.n.05.sister')], [Lemma('baby.n.06.baby')], [Lemma('baby.n.07.baby')], [Lemma('pamper.v.01.pamper'), Lemma('pamper.v.01.featherbed'), Lemma('pamper.v.01.cosset'), Lemma('pamper.v.01.cocker'), Lemma('pamper.v.01.baby'), Lemma('pamper.v.01.coddle'), Lemma('pamper.v.01.mollycoddle'), Lemma('pamper.v.01.spoil'), Lemma('pamper.v.01.indulge')]]\n",
      "[[Lemma('television.n.01.television'), Lemma('television.n.01.telecasting'), Lemma('television.n.01.TV'), Lemma('television.n.01.video')], [Lemma('television.n.02.television'), Lemma('television.n.02.television_system')], [Lemma('television_receiver.n.01.television_receiver'), Lemma('television_receiver.n.01.television'), Lemma('television_receiver.n.01.television_set'), Lemma('television_receiver.n.01.tv'), Lemma('television_receiver.n.01.tv_set'), Lemma('television_receiver.n.01.idiot_box'), Lemma('television_receiver.n.01.boob_tube'), Lemma('television_receiver.n.01.telly'), Lemma('television_receiver.n.01.goggle_box')]]\n",
      "[[Lemma('game.n.01.game')], [Lemma('game.n.02.game')], [Lemma('game.n.03.game')], [Lemma('game.n.04.game')], [Lemma('game.n.05.game')], [Lemma('game.n.06.game')], [Lemma('game.n.07.game')], [Lemma('plot.n.01.plot'), Lemma('plot.n.01.secret_plan'), Lemma('plot.n.01.game')], [Lemma('game.n.09.game')], [Lemma('game.n.10.game'), Lemma('game.n.10.biz')], [Lemma('game.n.11.game')], [Lemma('bet_on.v.01.bet_on'), Lemma('bet_on.v.01.back'), Lemma('bet_on.v.01.gage'), Lemma('bet_on.v.01.stake'), Lemma('bet_on.v.01.game'), Lemma('bet_on.v.01.punt')], [Lemma('crippled.s.01.crippled'), Lemma('crippled.s.01.halt'), Lemma('crippled.s.01.halting'), Lemma('crippled.s.01.lame'), Lemma('crippled.s.01.gimpy'), Lemma('crippled.s.01.game')], [Lemma('game.s.02.game'), Lemma('game.s.02.gamy'), Lemma('game.s.02.gamey'), Lemma('game.s.02.gritty'), Lemma('game.s.02.mettlesome'), Lemma('game.s.02.spirited'), Lemma('game.s.02.spunky')]]\n",
      "[[Lemma('currency.n.01.currency')], [Lemma('currency.n.02.currency')], [Lemma('currentness.n.01.currentness'), Lemma('currentness.n.01.currency'), Lemma('currentness.n.01.up-to-dateness')]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirstWord</th>\n",
       "      <th>SecondWord</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day</td>\n",
       "      <td>sidereal_day</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>malcolm_stock</td>\n",
       "      <td>stock</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sidereal_day</td>\n",
       "      <td>day</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>stock</td>\n",
       "      <td>malcolm_stock</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>area</td>\n",
       "      <td>country</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FirstWord     SecondWord  Similarity\n",
       "5             day   sidereal_day    0.333333\n",
       "42  malcolm_stock          stock    0.333333\n",
       "12   sidereal_day            day    0.333333\n",
       "41          stock  malcolm_stock    0.333333\n",
       "19           area        country    0.250000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict={'FirstWord':[],'SecondWord':[],'Similarity':[]}\n",
    "for word in word_list:\n",
    "    print([lemma for lemma in [sense.lemmas() for sense in wn.synsets(word)]])\n",
    "    for syns in wn.synsets(word):\n",
    "#         print(syns.name().split('.')[0])\n",
    "        maxsimilarity=['',None,0]\n",
    "        maxsimilarity[0]=wn.synset(syns.name())\n",
    "        for syns_inner in wn.synsets(word):\n",
    "            if syns.name().split('.')[0]!=syns_inner.name().split('.')[0]:\n",
    "                word2=wn.synset(syns_inner.name())\n",
    "                similarity=maxsimilarity[0].path_similarity(word2) or 0\n",
    "                if similarity>maxsimilarity[2]:\n",
    "                    maxsimilarity[1]=word2\n",
    "                    maxsimilarity[2]=similarity\n",
    "#         print(type(maxsimilarity[1]))\n",
    "        if maxsimilarity[1] is not None:\n",
    "            word_dict['FirstWord'].append(maxsimilarity[0].name().split('.')[0])\n",
    "            word_dict['SecondWord'].append(maxsimilarity[1].name().split('.')[0])\n",
    "            word_dict['Similarity'].append(maxsimilarity[2])\n",
    "#         print(word_dict)\n",
    "word_df=pd.DataFrame(word_dict,columns=['FirstWord','SecondWord','Similarity'])\n",
    "word_df.sort_values('Similarity',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirstWord</th>\n",
       "      <th>SecondWord</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>stock.n.12</td>\n",
       "      <td>malcolm_stock.n.01</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>day.n.07</td>\n",
       "      <td>day.n.04</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>stock.n.03</td>\n",
       "      <td>stock.n.15</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>stock.v.04</td>\n",
       "      <td>stock.v.03</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>stock.v.05</td>\n",
       "      <td>stock.v.03</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FirstWord          SecondWord  Similarity\n",
       "51  stock.n.12  malcolm_stock.n.01    0.333333\n",
       "11    day.n.07            day.n.04    0.333333\n",
       "42  stock.n.03          stock.n.15    0.333333\n",
       "60  stock.v.04          stock.v.03    0.333333\n",
       "61  stock.v.05          stock.v.03    0.333333"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(word_df.sort_values)\n",
    "word_df.sort_values('Similarity',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f9986faddfe7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msyns\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0msyns_inner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0mword2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyns_inner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                 \u001b[0msimilarity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxsimilarity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mmaxsimilarity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[0mmaxsimilarity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mpath_similarity\u001b[1;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m         distance = self.shortest_path_distance(\n\u001b[1;32m--> 830\u001b[1;33m             \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimulate_root\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_needs_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m         )\n\u001b[0;32m    832\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdistance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdistance\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_needs_root\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_needs_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mNOUN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wordnet_corpus_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1.6'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mget_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mADJ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1274\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1275\u001b[0m             \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'WordNet (\\d+\\.\\d+) Copyright'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1272\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;34m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m             \u001b[0mstartpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytebuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m             \u001b[0mnew_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m             \u001b[1;31m# If we're at a '\\r', then read one extra character, since\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1452\u001b[0m             \u001b[0mnew_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1454\u001b[1;33m             \u001b[0mnew_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1455\u001b[0m         \u001b[0mbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytebuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnew_bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_dict={'FirstWord':[],'SecondWord':[],'Similarity':[]}\n",
    "for word in word_list:\n",
    "    for syns in wn.synsets(word):\n",
    "        maxsimilarity=[0,0,0]\n",
    "        maxsimilarity[0]=wn.synset(syns.name())\n",
    "        for syns_inner in wn.all_synsets():\n",
    "            if syns!=syns_inner:\n",
    "                word2=wn.synset(syns_inner.name())\n",
    "                similarity=maxsimilarity[0].path_similarity(word2) or 0\n",
    "                if similarity>maxsimilarity[2]:\n",
    "                    maxsimilarity[1]=word2\n",
    "                    maxsimilarity[2]=similarity\n",
    "        word_dict['FirstWord'].append([0])\n",
    "        word_dict['SecondWord'].append([1])\n",
    "        word_dict['Similarity'].append([2])\n",
    "        print(word_dict)\n",
    "word_df=pd.DataFrame(word_dict,columns=['FirstWord','SecondWord','Similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('computer.n.01')\n",
      "Synset('calculator.n.01')\n"
     ]
    }
   ],
   "source": [
    "for x in wn.synsets('computer'):\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
