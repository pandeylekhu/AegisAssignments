{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDZh3S41yTw7",
        "colab_type": "text"
      },
      "source": [
        "<h3>Import all the required packages</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMBHmWnsykrp",
        "colab_type": "code",
        "outputId": "87a7e6b3-994f-4bcb-829a-36151a3fa14c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!pip install pyspellchecker\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/62/e9da86d71e3ccc500b979f0afb88c1f3ae151766004a0de92775b686a311/pyspellchecker-0.5.2-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.2\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ9HeUiUyTw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from spellchecker import SpellChecker\n",
        "from datetime import datetime\n",
        "import re,string\n",
        "training_chart=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QDFN5blyTxE",
        "colab_type": "text"
      },
      "source": [
        "<h3> This procedure will change the dual words to actual words line you'll to You will</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY9kbeJFyTxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def change_dual_words(word):\n",
        "    first=''\n",
        "    second=''\n",
        "    def get_correct_word(w):\n",
        "        if w==\"sha\":\n",
        "            return(\"shall\")\n",
        "        elif w==\"wo\":\n",
        "            return(\"will\")\n",
        "        elif w==\"y\":\n",
        "            return(\"you\")\n",
        "        else:\n",
        "            return(w)\n",
        "    word=word.strip().strip(\"'\").strip(',').strip('`').strip('[').strip(']').strip('-').strip('_').strip('#').strip('+').strip('/').strip('(').strip(')').lower()\n",
        "    if word.endswith(\"n't\"):\n",
        "        first=get_correct_word(word[:word.index(\"n't\")])\n",
        "        second=\"not\"\n",
        "    elif word.endswith(\"'ll\"):\n",
        "        first=get_correct_word(word[:word.index(\"'ll\")])\n",
        "        second=\"will\"\n",
        "    elif word.endswith(\"'ve\"):\n",
        "        first=get_correct_word(word[:word.index(\"'ve\")])\n",
        "        second=\"have\"\n",
        "    elif word.endswith(\"'re\"):\n",
        "        first=get_correct_word(word[:word.index(\"'re\")])\n",
        "        second=\"are\"\n",
        "    elif word.endswith(\"'d\"):\n",
        "        first=get_correct_word(word[:word.index(\"'d\")])\n",
        "        second=\"did\"\n",
        "    elif word.endswith(\"'s\"):\n",
        "        first=get_correct_word(word[:word.index(\"'s\")])\n",
        "        second=\"is\"\n",
        "    elif word.endswith(\"'all\"):\n",
        "        first=get_correct_word(word[:word.index(\"'all\")])\n",
        "        second=\"all\"\n",
        "    else:\n",
        "        first=word\n",
        "        second=\"<allclear>\"\n",
        "    return((first,second))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXqymgPAyTxK",
        "colab_type": "text"
      },
      "source": [
        "<h3>Clean the lines</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meUnU3DxyTxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanLine(line):\n",
        "#     stopword = stopwords.words('english')\n",
        "    stopword=['aa','aah','aahhs','aahs','(',')',\"''\",',','``','-','.','4','2','5','9','26','$$','$@^@','&#','&nbsp','&quot','#@%$^','==','>from','>honk','@#$^%','@#%','@$&%','a&m','a&w','a+']\n",
        "    stopwordList=set(stopword)\n",
        "    tokenizer = RegexpTokenizer('\\w+-+\\w+')\n",
        "    hyphenated_word=tokenizer.tokenize(line)\n",
        "#     print(hyphenated_word)\n",
        "    for word in hyphenated_word:\n",
        "        line=line.replace(word,word.replace('-',' '))\n",
        "    \n",
        "    tokenizer = RegexpTokenizer('\\w+-\\w+')\n",
        "    hyphenated_word=tokenizer.tokenize(line)\n",
        "#     print(hyphenated_word)\n",
        "    for word in hyphenated_word:\n",
        "        line=line.replace(word,word.replace('-',' '))\n",
        "        \n",
        "    tokenizer = RegexpTokenizer('\\w+_\\w+')\n",
        "    hyphenated_word=tokenizer.tokenize(line)\n",
        "#     print(hyphenated_word)\n",
        "    for word in hyphenated_word:\n",
        "        line=line.replace(word,word.replace('_',' '))\n",
        "        \n",
        "    tokenizer = RegexpTokenizer('\\w+/\\w+')\n",
        "    hyphenated_word=tokenizer.tokenize(line)\n",
        "#     if len(hyphenated_word)>0:print(hyphenated_word)\n",
        "    for word in hyphenated_word:\n",
        "        line=line.replace(word,word.replace('/',' '))\n",
        "        \n",
        "    tokenizer = RegexpTokenizer('\\w+/+\\w+')\n",
        "    hyphenated_word=tokenizer.tokenize(line)\n",
        "#     if len(hyphenated_word)>0:print(hyphenated_word)\n",
        "    for word in hyphenated_word:\n",
        "        line=line.replace(word,word.replace('/',' '))\n",
        "    \n",
        "    tokenizer = RegexpTokenizer('\\w+/\\'\\w+')\n",
        "    hyphenated_word=tokenizer.tokenize(line)\n",
        "#     if len(hyphenated_word)>0:print(hyphenated_word)\n",
        "    for word in hyphenated_word:\n",
        "        line=line.replace(word,word.replace('/\\'',' '))\n",
        "        \n",
        "#     tokenizer = RegexpTokenizer('\\w+\\'\\w+')\n",
        "#     hyphenated_word=tokenizer.tokenize(line)\n",
        "# #     if len(hyphenated_word)>0:print(hyphenated_word)\n",
        "#     for word in hyphenated_word:\n",
        "#         line=line.replace(word,word.replace('\\'',' '))\n",
        "\n",
        "    line=line.replace('y2k','the year 2000')\n",
        "    word_lst=line.split()\n",
        "    clean_lst=[]\n",
        "    for idx in range(0,len(word_lst)):\n",
        "        if word_lst[idx] not in punctuation and word_lst[idx] not in stopwordList:\n",
        "#             clean_lst.append(word_lst[idx])\n",
        "            dual_words=change_dual_words(word_lst[idx])\n",
        "            if dual_words[1]=='<allclear>':\n",
        "                    clean_lst.append(dual_words[0])\n",
        "            else:\n",
        "                    clean_lst.append(dual_words[0])\n",
        "                    clean_lst.append(dual_words[1])\n",
        "#     processed_lst.append('</s>')\n",
        "    return(str(' ').join(clean_lst))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcOokDI1yTxQ",
        "colab_type": "text"
      },
      "source": [
        "<h3> Change incorrect words and remove numbers</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jjO7yvlyTxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_words(line):\n",
        "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
        "    line=pattern.sub(r\"\\1\\1\",line)\n",
        "    line=line.replace('( see ratings chart below )','')\n",
        "    line = ''.join([x if x in string.printable else '' for x in line])\n",
        "        \n",
        "    alphanum=re.findall(r'\\(\\s*\\d*/10\\s*\\)|\\d/10',line)\n",
        "    for rating in alphanum:\n",
        "        rates=re.findall(r'\\d+/',rating)\n",
        "        try:\n",
        "            line=line.replace(rating,str(rates[0][:-1])+'R')\n",
        "        except IndexError as IE:\n",
        "            None\n",
        " \n",
        "    alphanum=set(re.findall(r'\\b\\d+\\s+bucks\\b|\\$\\d+',line))\n",
        "    for money in set(alphanum):\n",
        "        line=line.replace(money,'money')\n",
        "            \n",
        "    alphanum=re.findall(r'\\b\\d+\\'s\\b|\\b\\d{2}s\\b|\\b\\d{4}s\\b',line)\n",
        "    for year in alphanum:\n",
        "        if len(year[:-1].strip(\"'\"))==2:\n",
        "            ryear='19'+year[:-1].strip(\"'\")#+'Y'\n",
        "        else:\n",
        "            ryear=year[:-1].strip(\"'\")#+'Y'\n",
        "        if int(ryear)<=2019:\n",
        "            rstr='past years'\n",
        "        else:\n",
        "            rstr='in future'\n",
        "        line=line.replace(year,rstr)\n",
        "\n",
        "    alphanum=[x.strip().strip('/').strip('-') for x in re.findall(r'\\b\\d{4}[^RY]?\\b',line)]\n",
        "    for year in set(alphanum):\n",
        "        if int(year.strip())<=2019:\n",
        "            rstr='past years'\n",
        "        else:\n",
        "            rstr='in future'\n",
        "        line=line.replace(year.strip(),rstr)\n",
        "\n",
        "    alphanum=set(re.findall(r'\\b\\d+\\s+minutes\\b',line))\n",
        "    for time in set(alphanum):\n",
        "        line=line.replace(time,'minutes')\n",
        "        \n",
        "    alphanum=set(re.findall(r'\\b\\d+hr\\b',line))\n",
        "#     if len(alphanum)>0:print(alphanum)\n",
        "    for time in set(alphanum):\n",
        "        line=line.replace(time,'hour')\n",
        "        \n",
        "    alphanum=re.findall(r'\\b\\d+th\\b|\\b\\d+st\\b',line)\n",
        "#     if len(alphanum)>0:print(alphanum)\n",
        "    for digi in set(alphanum):\n",
        "        line=line.replace(digi,'nth')  \n",
        "        \n",
        "    alphanum=re.findall(r'\\b\\d+[^RY]\\b|\\b\\d+\\b',line)\n",
        "    for digi in set(alphanum):\n",
        "        line=line.replace(str(digi),'')\n",
        "        \n",
        "    return(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lacb6p1ByTxW",
        "colab_type": "text"
      },
      "source": [
        "<h3>Main calls for Data Collection, Data Clean UP and Data Engineering</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY0MX0TkyTxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "  movie_reviews_data=[]\n",
        "  for x in movie_reviews.fileids():\n",
        "      movr_fil=movie_reviews.open(x)\n",
        "      if x.startswith('neg'):\n",
        "          review=0\n",
        "      else:\n",
        "          review=1\n",
        "      for line in movr_fil.readlines():\n",
        "          if len(line.split())<5:\n",
        "              continue\n",
        "#         line=correct_words(line)    \n",
        "#         movie_reviews_data.append([cleanLine(line.strip()),review])\n",
        "          movie_reviews_data.append([line.strip(),review])\n",
        "  random.shuffle(movie_reviews_data)\n",
        "  print(datetime.now(),': Data Loaded...')\n",
        "  movie_reviews_df=pd.DataFrame(movie_reviews_data[:15000],columns=['COMMENTS','TAG'])\n",
        "  movie_reviews_df.COMMENTS=movie_reviews_df.COMMENTS.apply(lambda line:correct_words(line))\n",
        "  movie_reviews_df.COMMENTS=movie_reviews_df.COMMENTS.apply(lambda line:cleanLine(line))\n",
        "  print(datetime.now(),': Data Cleaned...')\n",
        "  spell=SpellChecker()\n",
        "  mispelled=('absolete','absoloute', 'absoltuely','amblyn','american','opportunety','ambivlaent','alzhiemer','alchoholic','aggressivelly','abundence','accomodates','actualy','adandon','admiting','admitedly','aformentioned','afterword','agee')\n",
        "  for word in mispelled:\n",
        "      movie_reviews_df.COMMENTS=movie_reviews_df.COMMENTS.str.replace(word,spell.correction(word))\n",
        "  print(datetime.now(),': Misspelled words corrected...')\n",
        "  return movie_reviews_df\n",
        "\n",
        "def lemmetize_data(movie_reviews_df):\n",
        "  stopword = stopwords.words('english')\n",
        "  wnl = WordNetLemmatizer()\n",
        "  movie_reviews_df.COMMENTS=movie_reviews_df.COMMENTS.str.split().apply(lambda word_lst: str(' ').join([wnl.lemmatize(word) if word not in stopword else word for word in word_lst]))\n",
        "  print(datetime.now(),': Words Lemmatized..')\n",
        "  return movie_reviews_df\n",
        "# snowball_stemmer = SnowballStemmer('english')\n",
        "# movie_reviews_df.COMMENTS=movie_reviews_df.COMMENTS.apply(lambda line: str(' ').join([snowball_stemmer.stem(word) if word not in stopword else word for word in line.split()]))\n",
        "# print('Words Stemmed..')\n",
        "\n",
        "def get_unigrams(movie_reviews_df):\n",
        "  stopword = stopwords.words('english')\n",
        "  stopword.extend(['(',')',\"''\",',','``','-','.','2','5','9','26'])\n",
        "  stopwordList=set(stopword)\n",
        "# tot_word_lst=[word for word_lst in movie_reviews_df.COMMENTS.str.split() for word in word_lst if word not in stopwordList]\n",
        "# print(datetime.now(),\": Tot no of words: \",len(tot_word_lst))\n",
        "  unigrams=movie_reviews_df.COMMENTS.str.lower().str.split(expand=True).stack().unique()\n",
        "  for x in stopwordList:\n",
        "      unigrams=np.delete(unigrams,np.argwhere(unigrams==x))\n",
        "  print(datetime.now(),\": Length of unigrams: \", unigrams.shape[0])\n",
        "  print(datetime.now(),': Unigrams Collected..')\n",
        "  return unigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isNljlXX_0v7",
        "colab_type": "code",
        "outputId": "215794a0-5a80-48ac-d963-683460be67a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "movie_reviews_df=lemmetize_data(get_data())\n",
        "unigrams=get_unigrams(movie_reviews_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-12 18:31:53.312087 : Data Loaded...\n",
            "2019-10-12 18:31:55.906790 : Data Cleaned...\n",
            "2019-10-12 18:31:57.280504 : Misspelled words corrected...\n",
            "2019-10-12 18:32:00.111678 : Words Lemmatized..\n",
            "2019-10-12 18:32:00.785749 : Length of unigrams:  19546\n",
            "2019-10-12 18:32:00.786374 : Unigrams Collected..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLoIqmO3yTxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_precision(prediction, actual):\n",
        "    predictions = list(prediction)\n",
        "    actuals= list(actual)\n",
        "    correct_labels = [predictions[i]  for i in range(len(predictions)-1) if actuals[i] == predictions[i]]\n",
        "    precision = float(len(correct_labels))/float(len(predictions))\n",
        "    return precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD1B_ZPlyTxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_models(training_features,training_labels,test_features,test_labels):\n",
        "    idx=len(training_chart)-1\n",
        "    # Naive Bayes Classifier \n",
        "    nb_classifier = MultinomialNB().fit(training_features,training_labels) #training process\n",
        "    predictions = nb_classifier.predict(test_features)\n",
        "    print(\"Precision of NB classifier is\")\n",
        "    precision = calculate_precision(predictions,test_labels)\n",
        "    print('NB:',precision)\n",
        "    training_chart[idx].append(str(precision))\n",
        "    # SVM Classifier\n",
        "    svm_classifier = LinearSVC(penalty='l2', C=0.01).fit(training_features,training_labels)\n",
        "    print(\"Precision of linear SVM classifier is:\")\n",
        "    predictions = svm_classifier.predict(test_features)\n",
        "    precision = calculate_precision(predictions,test_labels)\n",
        "    print(\"SVM:\" + str(precision))\n",
        "    training_chart[idx].append(str(precision))\n",
        "    #Decision Tree\n",
        "    dec_classifier = DecisionTreeClassifier(random_state=0).fit(training_features,training_labels)\n",
        "    print(\"Precision of Decision Tree classifier is:\")\n",
        "    predictions = dec_classifier.predict(test_features)\n",
        "    precision = calculate_precision(predictions,test_labels)\n",
        "    print(\"DTC:\" + str(precision))\n",
        "    training_chart[idx].append(str(precision))\n",
        "  # Logistic Regression\n",
        "    logReg = LogisticRegression().fit(training_features,training_labels)\n",
        "    print(\"Precision of Logistic Regression is:\")\n",
        "    predictions = logReg.predict(test_features)\n",
        "    precision = calculate_precision(predictions,test_labels)\n",
        "    print(\"LR:\" + str(precision))\n",
        "    training_chart[idx].append(str(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX9o6U-kyTx1",
        "colab_type": "code",
        "outputId": "4df962ff-629b-41fd-a544-fda7a428d8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "print('Getting Features...')\n",
        "unigram_data=movie_reviews_df.COMMENTS.str.lower().apply(lambda x: [1 if gram in x else 0 for gram in unigrams]).apply(pd.Series)\n",
        "print('Spliting data to test train....')\n",
        "training_chart=[]\n",
        "training_chart.append(['Unigram','OneHot'])\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(unigram_data,movie_reviews_df.TAG,test_size=0.30,random_state=0)\n",
        "print('Model Training in progress...')\n",
        "train_models(xtrain,ytrain,xtest,ytest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting Features...\n",
            "Spliting data to test train....\n",
            "Model Training in progress...\n",
            "Precision of NB classifier is\n",
            "NB: 0.6391111111111111\n",
            "Precision of linear SVM classifier is:\n",
            "SVM:0.6164444444444445\n",
            "Precision of Decision Tree classifier is:\n",
            "DTC:0.5333333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision of Logistic Regression is:\n",
            "LR:0.6273333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGCNNDcZyTx_",
        "colab_type": "code",
        "outputId": "afb3a1fa-5b83-4799-b226-1dd8c77f9d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "unigram_data=''\n",
        "print('Getting Features....')\n",
        "unigram_data=movie_reviews_df.COMMENTS.str.lower().apply(lambda x: [x.count(gram) if gram in x else 0 for gram in unigrams]).apply(pd.Series)\n",
        "print('Spliting data to test train....')\n",
        "training_chart.append(['Unigram','Frequency'])\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(unigram_data,movie_reviews_df.TAG,test_size=0.30,random_state=0)\n",
        "print('Model Training in progress...')\n",
        "train_models(xtrain,ytrain,xtest,ytest)\n",
        "training_chart_df=pd.DataFrame(training_chart,columns=['Features','FeatureWeight','NB','SVM','DTC','LR'])\n",
        "training_chart_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting Features....\n",
            "Spliting data to test train....\n",
            "Model Training in progress...\n",
            "Precision of NB classifier is\n",
            "NB: 0.6348888888888888\n",
            "Precision of linear SVM classifier is:\n",
            "SVM:0.6217777777777778\n",
            "Precision of Decision Tree classifier is:\n",
            "DTC:0.536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision of Logistic Regression is:\n",
            "LR:0.6277777777777778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Features</th>\n",
              "      <th>FeatureWeight</th>\n",
              "      <th>NB</th>\n",
              "      <th>SVM</th>\n",
              "      <th>DTC</th>\n",
              "      <th>LR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Unigram</td>\n",
              "      <td>OneHot</td>\n",
              "      <td>0.6391111111111111</td>\n",
              "      <td>0.6164444444444445</td>\n",
              "      <td>0.5333333333333333</td>\n",
              "      <td>0.6273333333333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Unigram</td>\n",
              "      <td>Frequency</td>\n",
              "      <td>0.6348888888888888</td>\n",
              "      <td>0.6217777777777778</td>\n",
              "      <td>0.536</td>\n",
              "      <td>0.6277777777777778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Features FeatureWeight  ...                 DTC                  LR\n",
              "0  Unigram        OneHot  ...  0.5333333333333333  0.6273333333333333\n",
              "1  Unigram     Frequency  ...               0.536  0.6277777777777778\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow6t-dvpTkUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from textblob import TextBlob as tb\n",
        "\n",
        "def tf(word, blob):\n",
        "    return blob.words.count(word) / len(blob.words)\n",
        "\n",
        "def n_containing(word, bloblist):\n",
        "    return sum(1 for blob in bloblist if word in blob.words)\n",
        "\n",
        "def idf(word, bloblist):\n",
        "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
        "\n",
        "def tfidf(word, blob, bloblist):\n",
        "    return tf(word, blob) * idf(word, bloblist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p75pjpGdUiPj",
        "colab_type": "code",
        "outputId": "8d1dfe74-200f-4b42-e807-0aefaf1ec910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# bloblist=[tb(line) for line in movie_reviews_df.COMMENTS.str.lower()]\n",
        "# unigram_data=movie_reviews_df.COMMENTS.str.lower().apply(lambda x: [x.count(gram) if gram in x else 0 for gram in unigrams]).apply(pd.Series)\n",
        "\n",
        "movie_reviews_df.COMMENTS.str.lower().apply(lambda line:tb(str(line)))\n",
        "# document3 = tb(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
        "# manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
        "# It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
        "# in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
        "# Colt Python targeted the premium revolver market segment. Some firearm\n",
        "# collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
        "# Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
        "# finest production revolver ever made.\"\"\")\n",
        "# document3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        (i, n,  , t, h, e,  , w, e, a, k, e, s, t,  , ...\n",
              "1        (b, e, a, t, t, y,  , p, l, a, y,  , i, n, c, ...\n",
              "2        (s, e, t, h, e,  , i, s,  , p, r, o, u, d,  , ...\n",
              "3        (m, c, f, a, r, l, a, n, e,  , c, r, e, a, t, ...\n",
              "4        (w, o, r, s, t,  , o, f,  , a, l, l,  , g, o, ...\n",
              "5        (t, h, e,  , s, e, a, s, o, n, e, d,  , c, a, ...\n",
              "6        (u, n, f, o, r, t, u, n, a, t, e, l, y,  , m, ...\n",
              "7        (b, u, t,  , i, t,  , c, o, u, l, d, n,  , t, ...\n",
              "8        (s, p, e, a, k, i, n, g,  , o, f,  , a, c, t, ...\n",
              "9        (t, h, e, y,  , h, a, v, e,  , s, o,  , m, u, ...\n",
              "10       (r, a, r, e, l, y,  , a, n, y, m, o, r, e,  , ...\n",
              "11       (t, h, i, s,  , i, s,  , p, r, e, t, t, y,  , ...\n",
              "12       (i, n,  , t, h, e,  , m, i, d, s, t,  , o, f, ...\n",
              "13       (a,  , r, o, b, o, t, i, c,  , b, o, y,  , n, ...\n",
              "14       (t, h, e,  , f, i, l, m,  , i, s,  , a,  , t, ...\n",
              "15       (h, e,  , v, i, s, i, t,  , a, n, d,  , s, o, ...\n",
              "16       (w, e,  , d, o,  , n, o, t,  , r, e, a, l, l, ...\n",
              "17       (b, a, r, b,  , w, i, r, e,  , p, a, m, e, l, ...\n",
              "18       (i, t,  , s, e, e, m, s,  , m, o, r, e,  , l, ...\n",
              "19       (i, t,  , c, r, e, a, t, e, s,  , a,  , w, o, ...\n",
              "20       (s, i, n, c, e,  , d, i, r, e, c, t, o, r,  , ...\n",
              "21       (i, t,  , i, s,  , n, o,  , g, o, o, d, f, e, ...\n",
              "22       (h, e,  , l, i, k, e,  , t, h, e,  , d, i, s, ...\n",
              "23       (t, w, o,  , l, o, w,  , k, e, y,  , s, u, m, ...\n",
              "24       (i, s,  , i, t,  , t, h, e,  , s, c, r, i, p, ...\n",
              "25       (i, t,  , o, f, f, e, r,  , s, o, m, e, t, h, ...\n",
              "26       (t, h, e, y,  , c, l, a, i, m,  , t, h, a, t, ...\n",
              "27       (i, t,  , t, a, k, e,  , f, o, r, e, v, e, r, ...\n",
              "28       (u, n, f, o, r, t, u, n, a, t, e, l, y,  , s, ...\n",
              "29       (t, h, e,  , t, w, o,  , f, a, m, i, l, y,  , ...\n",
              "                               ...                        \n",
              "14970    (a, m, a, z, i, n, g, l, y,  , o, n,  , w, h, ...\n",
              "14971    (m, a, n, y,  , o, f,  , w, h, i, c, h,  , h, ...\n",
              "14972    (t, h, e,  , f, i, l, m,  , i, s,  , n, o, t, ...\n",
              "14973    (t, h, i, s,  , i, s,  , b, e, s, t,  , e, x, ...\n",
              "14974    (w, e,  , d, o,  , n, o, t,  , r, e, a, l, l, ...\n",
              "14975    (h, e,  , h, a, s,  , c, o, m, e,  , b, a, c, ...\n",
              "14976    (b, l, a, d, e,  , i, s,  , b, a, s, e, d,  , ...\n",
              "14977                 (s, t, e, v, e,  , m, a, r, t, i, n)\n",
              "14978    (t, h, e, i, r,  , c, o, m, p, a, n, y,  , i, ...\n",
              "14979    (w, a, l, k, i, n, g,  , t, h, r, o, u, g, h, ...\n",
              "14980    (l, o, s, t,  , i, n,  , t, h, e,  , i, n, u, ...\n",
              "14981    (i, t,  , m, u, s, t,  , b, e,  , t, o, u, g, ...\n",
              "14982    (b, o, t, h,  , a, r, e,  , w, i, s, e, c, r, ...\n",
              "14983    (b, o, t, t, o, m,  , l, i, n, e,  , t, h, e, ...\n",
              "14984    (s, c, e, n, e,  , p, i, l, e,  , u, p,  , l, ...\n",
              "14985    (w, h, a, t,  , a,  , s, h, o, c, k,  , t, o, ...\n",
              "14986    (w, i, t, h,  , h, e, r,  , h, u, s, b, a, n, ...\n",
              "14987    (b, u, t,  , a, l, s, o,  , l, i, k, e,  , t, ...\n",
              "14988    (t, r, o, u, b, l, e,  , s, e, e, m, s,  , t, ...\n",
              "14989    (s, o, m, e, t, i, m, e, s,  , a, f, t, e, r, ...\n",
              "14990    (a, s,  , t, h, e,  , s, t, r, a, n, g, e, r, ...\n",
              "14991    (n, o, t,  , t, h, a, t,  , t, h, e,  , c, h, ...\n",
              "14992    (b, u, t,  , i, n, s, t, e, a, d,  , o, f,  , ...\n",
              "14993    (c, o, n, n, e, r, y,  , i, s,  , v, o, i, c, ...\n",
              "14994    (m, a, y, o, r,  , w, a, n, d, o,  , d, e, m, ...\n",
              "14995    (f, r, o, m,  , t, h, e,  , b, e, g, i, n, n, ...\n",
              "14996    (w, h, i, l, e,  , r, u, d, y,  , i, s,  , a, ...\n",
              "14997    (f, o, r,  , t, h, o, s, e,  , w, h, o,  , l, ...\n",
              "14998    (i, t,  , i, s,  , r, e, v, e, a, l, e, d,  , ...\n",
              "14999    (l, e, t,  , i, s,  , r, o, p, e,  , i, n,  , ...\n",
              "Name: COMMENTS, Length: 15000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRkSXmzNyTyP",
        "colab_type": "text"
      },
      "source": [
        "## Training Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOy5Lh_7yTyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Naive Bayes Classifier \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb_classifier = MultinomialNB().fit(training_features,training_labels) #training process\n",
        "predictions = nb_classifier.predict(test_features)\n",
        "print(\"Precision of NB classifier is\")\n",
        "precision = calculate_precision(predictions,test_gold_labels)\n",
        "print(precision)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXhJfzfDyTyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM Classifier\n",
        "#Refer to : http://scikit-learn.org/stable/modules/svm.html\n",
        "from sklearn.svm import LinearSVC\n",
        "svm_classifier = LinearSVC(penalty='l2', C=0.01).fit(training_features,training_labels)\n",
        "print(\"Precision of linear SVM classifier is:\")\n",
        "predictions = svm_classifier.predict(test_features)\n",
        "precision = calculate_precision(predictions,test_gold_labels)\n",
        "print(\"Test data\\t\" + str(precision))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuKPelUFyTyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dec_classifier = DecisionTreeClassifier(random_state=0).fit(training_features,training_labels)\n",
        "print(\"Precision of Decision Tree classifier is:\")\n",
        "predictions = dec_classifier.predict(test_features)\n",
        "precision = calculate_precision(predictions,test_gold_labels)\n",
        "print(\"Test data\\t\" + str(precision))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHSBb5pqyTyh",
        "colab_type": "text"
      },
      "source": [
        "#### Add sentiment scores from sentiwordnet, here we take the average sentiment scores of all words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_kfmhANyTyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_senti_wordnet_features(data):\n",
        "    fet_vec_all = []\n",
        "    for tup in data:\n",
        "        sent = tup[0].lower()\n",
        "        words = sent.split()\n",
        "        pos_score = 0\n",
        "        neg_score = 0\n",
        "        for w in words:\n",
        "            senti_synsets = swn.senti_synsets(w.lower())\n",
        "            for senti_synset in senti_synsets:\n",
        "                p = senti_synset.pos_score()\n",
        "                n = senti_synset.neg_score()\n",
        "                pos_score+=p\n",
        "                neg_score+=n\n",
        "                break #take only the first synset (Most frequent sense)\n",
        "        fet_vec_all.append([float(pos_score),float(neg_score)])\n",
        "    return fet_vec_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SCjd6k1yTyo",
        "colab_type": "text"
      },
      "source": [
        "#### Merge the two scores ####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3_HfjHkyTyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_features(featureList1,featureList2):\n",
        "    # For merging two features\n",
        "    if featureList1==[]:\n",
        "        return featureList2\n",
        "    merged = []\n",
        "    for i in range(len(featureList1)):\n",
        "        m = featureList1[i]+featureList2[i]\n",
        "        merged.append(m)\n",
        "    return merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd0nNuh2yTyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_unigram_features = get_unigram_features(training_data,unigrams) # vocabulary extracted in the beginning\n",
        "print(training_unigram_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhkfRCxyyTyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_swn_features = get_senti_wordnet_features(training_data)\n",
        "print(training_swn_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBvXZKtmyTy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_features = merge_features(training_unigram_features,training_swn_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDQJgvNYyTy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(training_features[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN0R2UcgyTy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_unigram_features = get_unigram_features(test_data,unigrams)\n",
        "test_swn_features=get_senti_wordnet_features(test_data)\n",
        "test_features= merge_features(test_unigram_features,test_swn_features)\n",
        "\n",
        "#training_labels = get_lables(training_data)\n",
        "#test_gold_labels = get_lables(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yld37of2yTzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For Mini-Project\n",
        "#1. Preprocessing : improve with lemma, stemming, POS, bigram\n",
        "#2. Feature scores : one/hot encoding (0/1), frequency, TF-IDF, probabity \n",
        "#3. Clasifiers: (NB, SVM, DT)\n",
        "#4. Normalization : having uniform form of words throught \n",
        "#5. Compare with VADER : https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}